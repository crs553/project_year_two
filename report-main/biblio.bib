@online{wiki:cifar,
author = "Wikipedia",
title = "{CIFAR-10}",
year = "2021",
url = "https://en.wikipedia.org/wiki/CIFAR-10",
addendum = "(last modified: 29 March 2021, 03:12)"
}
@online{demush:vision,
author = "Rostyslav Demush",
title = "{A Brief History of Computer Vision (and Convolutional Neural Networks)}",
year = "2019",
url = "https://hackernoon.com/a-brief-history-of-computer-vision-and-convolutional-neural-networks-8fe8aacc79f3",
addendum = "(accessed: 24 April 2021)"
}
@inproceedings{goodfellow:maxout,
title = "{Maxout Networks}",
author = {Ian Goodfellow and David Warde-Farley and Mehdi Mirza and Aaron Courville and Yoshua Bengio},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
pages = {1319--1327},
year = {2013},
editor = {Sanjoy Dasgupta and David McAllester},
volume = {28},
number = {3},
series = {Proceedings of Machine Learning Research},
address = {Atlanta, Georgia, USA},
month = {06},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v28/goodfellow13.pdf},
url = {http://proceedings.mlr.press/v28/goodfellow13.html},
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}
}
@inproceedings{krizhevsky:belief,
title = "{Convolutional Deep Belief Networks on CIFAR-10}",
author = "Alex Krizhevsky",
booktitle = "Unpublished manuscript",
year = "2010",
url = "https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf"
}
